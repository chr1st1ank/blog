{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_folder = Path(globals()['_dh'][0]) / \"..\" / \"data\"\n",
    "output_folder = Path(globals()['_dh'][0]) / \"output\"\n",
    "output_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>obrigado um milhão.</td>\n",
       "      <td>Portugeese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>നിങ്ങൾ ഒരു ടെലിഫോൺ കോളിൽ ഉപയോഗിക്കാൻ മികച്ച വാ...</td>\n",
       "      <td>Malayalam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>zevkle.</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>അത് നിർത്തിയപ്പോൾ അതിൽ നിന്ന് ഒരു സുന്ദരൻ പുറത...</td>\n",
       "      <td>Malayalam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>Википедия и её братские проекты провели широки...</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text    Language\n",
       "3180                                obrigado um milhão.  Portugeese\n",
       "1666  നിങ്ങൾ ഒരു ടെലിഫോൺ കോളിൽ ഉപയോഗിക്കാൻ മികച്ച വാ...   Malayalam\n",
       "8222                                            zevkle.     Turkish\n",
       "1951  അത് നിർത്തിയപ്പോൾ അതിൽ നിന്ന് ഒരു സുന്ദരൻ പുറത...   Malayalam\n",
       "6185  Википедия и её братские проекты провели широки...     Russian"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_folder / \"LanguageDetection.csv\", sep=\",\")\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples by language:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Language\n",
       "Hindi           63\n",
       "Greek          365\n",
       "Kannada        369\n",
       "Danish         428\n",
       "Tamil          469\n",
       "German         470\n",
       "Turkish        474\n",
       "Arabic         536\n",
       "Dutch          546\n",
       "Malayalam      594\n",
       "Sweedish       676\n",
       "Russian        692\n",
       "Italian        698\n",
       "Portugeese     739\n",
       "Spanish        819\n",
       "French        1014\n",
       "English       1385\n",
       "Name: Text, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of samples by language:\")\n",
    "data.groupby(by=[\"Language\"]).count()[\"Text\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"Text\"].values\n",
    "y = data[\"Language\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8269,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['τώρα αργότερα η Μέλι και ο Τέρι έσπασαν αντίο στον παλιό τους φίλο και πήγαν να χαμογελούν ο ένας στον άλλο κρυφά εκείνο το βράδυ, τόσο μητέρα όσο και κόρη.',\n",
       "       'Améliorez-le ou discutez-en.',\n",
       "       'non ne vale la pena, personalmente amo la frase you rock che significa che sei fantastico.',\n",
       "       ...,\n",
       "       'Si alguien te pregunta si estás cansado y quieres decir que no estás cansado en absoluto.',\n",
       "       'Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms.',\n",
       "       'Sono disponibili inoltre applicazioni dedicate per i vari dispositivi mobili, ad esempio Wikipedia Mobile per iPhone e Wikipedia Mobile per Android.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(lowercase=False, token_pattern='[\\\\w_]{2,}')),\n",
       "                ('naive_bayes', MultinomialNB(alpha=0.01))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(token_pattern=r\"[\\w_]{2,}\", lowercase=False)),\n",
    "    (\"naive_bayes\", MultinomialNB(alpha=.01)),\n",
    "])\n",
    "#clf_pipeline = Pipeline([\n",
    "#    (\"wordcount\", CountVectorizer()),\n",
    "#    (\"naive_bayes\", MultinomialNB(alpha=.01)),\n",
    "#])\n",
    "clf_pipeline.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction and sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.98      0.99       106\n",
      "      Danish       0.99      0.96      0.97        73\n",
      "       Dutch       0.99      0.98      0.99       111\n",
      "     English       0.92      1.00      0.96       291\n",
      "      French       1.00      0.99      0.99       219\n",
      "      German       1.00      0.98      0.99        93\n",
      "       Greek       1.00      1.00      1.00        68\n",
      "       Hindi       1.00      1.00      1.00        10\n",
      "     Italian       1.00      0.99      0.99       145\n",
      "     Kannada       1.00      1.00      1.00        66\n",
      "   Malayalam       1.00      0.98      0.99       121\n",
      "  Portugeese       0.99      0.97      0.98       144\n",
      "     Russian       1.00      0.99      0.99       136\n",
      "     Spanish       0.99      0.97      0.98       160\n",
      "    Sweedish       0.99      0.98      0.99       133\n",
      "       Tamil       1.00      0.99      0.99        87\n",
      "     Turkish       1.00      0.98      0.99       105\n",
      "\n",
      "    accuracy                           0.98      2068\n",
      "   macro avg       0.99      0.98      0.99      2068\n",
      "weighted avg       0.99      0.98      0.98      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf_pipeline.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein kleiner deutscher Text => German 0.768441150069108\n",
      "A small text without meaning => English 0.9986415618842468\n",
      "C'è un pò d'italiano => Italian 0.7141641875031932\n",
      "Une petite histoire de Paris => French 0.9968009803867346\n",
      "Генсек ООН призвал к соблюдению перемирия во время Олимпиады => Russian 0.9912020536752277\n"
     ]
    }
   ],
   "source": [
    "for sample in [\"Ein kleiner deutscher Text\", \"A small text without meaning\", \"C'è un pò d'italiano\", \"Une petite histoire de Paris\", \"Генсек ООН призвал к соблюдению перемирия во время Олимпиады\"]:\n",
    "    print(sample, \"=>\", clf_pipeline.predict(np.array([sample]))[0], np.max(clf_pipeline.predict_proba(np.array([sample]))[0]))\n",
    "    clf_pipeline.predict_proba(np.array([sample]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pickle the sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (output_folder / \"classifier.pickle\").open(\"wb\") as f:\n",
    "    pickle.dump(clf_pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert to and store Onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute intercept_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/skl2onnx/common/_container.py:607: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import skl2onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType\n",
    "\n",
    "tfidf_settings = {\n",
    "    TfidfVectorizer: {\n",
    "        \"tokenexp\": r\"[\\pL\\pN_]{2,}\"\n",
    "    }\n",
    "}\n",
    "initial_type = [('string_input', StringTensorType([None, 1]))]\n",
    "onx = convert_sklearn(clf_pipeline, initial_types=initial_type, options=tfidf_settings)\n",
    "with (output_folder / \"classifier.onnx\").open(\"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2M\t./output/classifier.onnx\r\n",
      "11M\t./output/classifier.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!du -ha ./output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in tokenization and normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, token_pattern='[İ\\\\w_]{2,}')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern=r\"[İ\\w_]{2,}\", lowercase=False)\n",
    "vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_settings = {\n",
    "    TfidfVectorizer: {\n",
    "        \"tokenexp\": r\"[\\pL\\pN_]{2,}\"\n",
    "    }\n",
    "}\n",
    "initial_type = [('string_input', StringTensorType([None, 1]))]\n",
    "onx = convert_sklearn(vectorizer, initial_types=initial_type, options=tfidf_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5373/2732278623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/vectorizer.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/vectorizer.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "with open(\"data/vectorizer.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n",
    "session = onnxruntime.InferenceSession(\"data/vectorizer.onnx\")\n",
    "inputs = {'string_input': x_test[:1]}\n",
    "pred_onx = session.run(None, {\"string_input\": np.array([\"И с этими словами она села в его карету, и, даже не\"]).reshape(1, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in x_train:\n",
    "    pred_sklearn = np.sum(vectorizer.transform(np.array([t])))\n",
    "    pred_onx = np.sum(session.run(None, {\"string_input\": np.array([t]).reshape(1, 1)}))\n",
    "    if abs(pred_onx - pred_sklearn) > 0.01:\n",
    "        print(t, pred_sklearn, pred_onx)\n",
    "        print(sorted(list(vectorizer.inverse_transform(vectorizer.transform(np.array([t])))[0])))\n",
    "        print(sorted(list(vectorizer.inverse_transform(session.run(None, {\"string_input\": np.array([t]).reshape(1, 1)})[0])[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction with ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict ['Russian']\n",
      "predict_proba [{'Arabic': 7.943455784698017e-06, 'Danish': 1.0405490684206598e-05, 'Dutch': 8.303594768221956e-06, 'English': 2.679041699593654e-06, 'French': 3.670612613859703e-06, 'German': 1.1265211469435599e-05, 'Greek': 1.0832965926965699e-05, 'Hindi': 1.1893760529346764e-05, 'Italian': 5.484678695211187e-06, 'Kannada': 1.8420292690279894e-05, 'Malayalam': 1.082171183952596e-05, 'Portugeese': 5.10070185555378e-06, 'Russian': 0.9998569488525391, 'Spanish': 5.064754532213556e-06, 'Sweedish': 6.081358151277527e-06, 'Tamil': 1.3298573321662843e-05, 'Turkish': 1.1620059012784623e-05}]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "session = onnxruntime.InferenceSession(str(output_folder / \"classifier.onnx\"))\n",
    "pred_onx = session.run(None, {\"string_input\": np.array([\"И с этими словами она села в его карету, и, даже не\"]).reshape(1, 1)})\n",
    "print(\"predict\", pred_onx[0])\n",
    "print(\"predict_proba\", pred_onx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein kleiner deutscher Text => ['German'] 0.7684409618377686\n",
      "A small text without meaning => ['English'] 0.9986410140991211\n",
      "C'è un pò d'italiano => ['Italian'] 0.7141642570495605\n",
      "Une petite histoire de Paris => ['French'] 0.9968007802963257\n",
      "Генсек ООН призвал к соблюдению перемирия во время Олимпиады => ['Russian'] 0.991202175617218\n"
     ]
    }
   ],
   "source": [
    "for sample in [\"Ein kleiner deutscher Text\", \"A small text without meaning\", \"C'è un pò d'italiano\", \"Une petite histoire de Paris\", \"Генсек ООН призвал к соблюдению перемирия во время Олимпиады\"]:\n",
    "    pred_onx = session.run(None, {\"string_input\": np.array([sample]).reshape(1, 1)})\n",
    "    print(sample, \"=>\", pred_onx[0], pred_onx[1][0][pred_onx[0][0]])\n",
    "    #print(\"predict_proba\", pred_onx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.98      0.99       106\n",
      "      Danish       0.99      0.96      0.97        73\n",
      "       Dutch       0.99      0.98      0.99       111\n",
      "     English       0.92      1.00      0.96       291\n",
      "      French       1.00      0.99      0.99       219\n",
      "      German       1.00      0.98      0.99        93\n",
      "       Greek       1.00      1.00      1.00        68\n",
      "       Hindi       1.00      1.00      1.00        10\n",
      "     Italian       1.00      0.99      0.99       145\n",
      "     Kannada       1.00      1.00      1.00        66\n",
      "   Malayalam       1.00      0.98      0.99       121\n",
      "  Portugeese       0.99      0.97      0.98       144\n",
      "     Russian       1.00      0.99      0.99       136\n",
      "     Spanish       0.99      0.97      0.98       160\n",
      "    Sweedish       0.99      0.98      0.99       133\n",
      "       Tamil       1.00      0.99      0.99        87\n",
      "     Turkish       1.00      0.98      0.99       105\n",
      "\n",
      "    accuracy                           0.98      2068\n",
      "   macro avg       0.99      0.98      0.99      2068\n",
      "weighted avg       0.99      0.98      0.98      2068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_onnx = session.run(None, {\"string_input\": np.array([s.encode(\"utf-8\") for s in x_test]).reshape(len(x_test), 1)})\n",
    "print(metrics.classification_report(y_test, pred_onnx[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample():\n",
    "    sample_size = 1\n",
    "    for i in range(0, len(x_test)-sample_size, sample_size):\n",
    "        yield x_test[i:i+sample_size]\n",
    "\n",
    "def benchmark_onnx():\n",
    "    for t in sample():\n",
    "        pred_onx = session.run(None, {\"string_input\": np.array([t]).reshape(len(t), 1)})\n",
    "\n",
    "def benchmark_sklearn():\n",
    "    for t in sample():\n",
    "        pred = clf_pipeline.predict(np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.04 s ± 920 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit benchmark_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.84 s ± 99.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit benchmark_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d06497f08ec5d0d372847304a386590630b6df8b8a02230ab933dab27f7787f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
