{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_folder = Path(globals()['_dh'][0]) / \"..\" / \"data\"\n",
    "output_folder = Path(globals()['_dh'][0]) / \"output\"\n",
    "output_folder.mkdir(exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial look at the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is a labeled list of short text fragments in 17 different languages. These are some examples of the data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv(data_folder / \"LanguageDetection.csv\", sep=\",\")\n",
    "data.sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   Text   Language\n",
       "5250  no te olvides de decir, ¿quién es ese tipo afa...    Spanish\n",
       "9271                          افعل ذلك شاكرا لكم مقدما.     Arabic\n",
       "1618  എന്റെ പ്ലേറ്റിൽ വളരെയധികം കാര്യങ്ങൾ ഞാൻ ജോലിയി...  Malayalam\n",
       "3657  Ainsi, en juin 2009, le philosophe français Be...     French\n",
       "2098  உதாரணமாக, ஆங்கிலப் பதிப்பில், பதிவுசெய்த பயனர்...      Tamil"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5250</th>\n",
       "      <td>no te olvides de decir, ¿quién es ese tipo afa...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>افعل ذلك شاكرا لكم مقدما.</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>എന്റെ പ്ലേറ്റിൽ വളരെയധികം കാര്യങ്ങൾ ഞാൻ ജോലിയി...</td>\n",
       "      <td>Malayalam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>Ainsi, en juin 2009, le philosophe français Be...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>உதாரணமாக, ஆங்கிலப் பதிப்பில், பதிவுசெய்த பயனர்...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution is of languages is relatively equal:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "samples = data.groupby(by=[\"Language\"]).count()[\"Text\"].sort_values()\n",
    "\n",
    "print(\"Number of samples by language:\")\n",
    "pd.DataFrame({\"count\": samples, \"percent\": (samples/sum(samples)).round(3)*100})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of samples by language:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            count  percent\n",
       "Language                  \n",
       "Hindi          63      0.6\n",
       "Greek         365      3.5\n",
       "Kannada       369      3.6\n",
       "Danish        428      4.1\n",
       "Tamil         469      4.5\n",
       "German        470      4.5\n",
       "Turkish       474      4.6\n",
       "Arabic        536      5.2\n",
       "Dutch         546      5.3\n",
       "Malayalam     594      5.7\n",
       "Sweedish      676      6.5\n",
       "Russian       692      6.7\n",
       "Italian       698      6.8\n",
       "Portugeese    739      7.1\n",
       "Spanish       819      7.9\n",
       "French       1014      9.8\n",
       "English      1385     13.4"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hindi</th>\n",
       "      <td>63</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greek</th>\n",
       "      <td>365</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kannada</th>\n",
       "      <td>369</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Danish</th>\n",
       "      <td>428</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tamil</th>\n",
       "      <td>469</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>470</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkish</th>\n",
       "      <td>474</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>536</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch</th>\n",
       "      <td>546</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malayalam</th>\n",
       "      <td>594</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweedish</th>\n",
       "      <td>676</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>692</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>698</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portugeese</th>\n",
       "      <td>739</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>819</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>1014</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>1385</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train/test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start our machine learning task by splitting the data into a train and a test set, so that we can later measure the performance:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x = data[\"Text\"].values\n",
    "y = data[\"Language\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ML pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to create a scikit-learn pipeline for the preprocessing and training. The two steps in the pipeline are the Tf-idf vectorizer and a Naive Bayes model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "clf_pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(token_pattern=r\"[\\w_]{2,}\", lowercase=False)),\n",
    "    (\"naive_bayes\", MultinomialNB(alpha=.01)),\n",
    "])\n",
    "#clf_pipeline = Pipeline([\n",
    "#    (\"wordcount\", CountVectorizer()),\n",
    "#    (\"naive_bayes\", MultinomialNB(alpha=.01)),\n",
    "#])\n",
    "clf_pipeline.fit(x_train, y_train)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(lowercase=False, token_pattern='[\\\\w_]{2,}')),\n",
       "                ('naive_bayes', MultinomialNB(alpha=0.01))])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test prediction and sanity check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pred = clf_pipeline.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.98      0.99       106\n",
      "      Danish       0.99      0.96      0.97        73\n",
      "       Dutch       0.99      0.98      0.99       111\n",
      "     English       0.92      1.00      0.96       291\n",
      "      French       1.00      0.99      0.99       219\n",
      "      German       1.00      0.98      0.99        93\n",
      "       Greek       1.00      1.00      1.00        68\n",
      "       Hindi       1.00      1.00      1.00        10\n",
      "     Italian       1.00      0.99      0.99       145\n",
      "     Kannada       1.00      1.00      1.00        66\n",
      "   Malayalam       1.00      0.98      0.99       121\n",
      "  Portugeese       0.99      0.97      0.98       144\n",
      "     Russian       1.00      0.99      0.99       136\n",
      "     Spanish       0.99      0.97      0.98       160\n",
      "    Sweedish       0.99      0.98      0.99       133\n",
      "       Tamil       1.00      0.99      0.99        87\n",
      "     Turkish       1.00      0.98      0.99       105\n",
      "\n",
      "    accuracy                           0.98      2068\n",
      "   macro avg       0.99      0.98      0.99      2068\n",
      "weighted avg       0.99      0.98      0.98      2068\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for sample in [\"Ein kleiner deutscher Text\", \"A small text without meaning\", \"C'è un pò d'italiano\", \"Une petite histoire de Paris\", \"Генсек ООН призвал к соблюдению перемирия во время Олимпиады\"]:\n",
    "    print(sample, \"=>\", clf_pipeline.predict(np.array([sample]))[0], np.max(clf_pipeline.predict_proba(np.array([sample]))[0]))\n",
    "    clf_pipeline.predict_proba(np.array([sample]))[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ein kleiner deutscher Text => German 0.768441150069108\n",
      "A small text without meaning => English 0.9986415618842468\n",
      "C'è un pò d'italiano => Italian 0.7141641875031932\n",
      "Une petite histoire de Paris => French 0.9968009803867346\n",
      "Генсек ООН призвал к соблюдению перемирия во время Олимпиады => Russian 0.9912020536752277\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serialize model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Pickle the sklearn model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "with (output_folder / \"classifier.pickle\").open(\"wb\") as f:\n",
    "    pickle.dump(clf_pipeline, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Convert to and store Onnx model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import StringTensorType\n",
    "\n",
    "tfidf_settings = {\n",
    "    TfidfVectorizer: {\n",
    "        \"tokenexp\": r\"[\\pL\\pN_]{2,}\"\n",
    "    }\n",
    "}\n",
    "initial_type = [('string_input', StringTensorType([None, 1]))]\n",
    "onx = convert_sklearn(clf_pipeline, initial_types=initial_type, options=tfidf_settings)\n",
    "  \n",
    "with (output_folder / \"classifier.onnx\").open(\"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute intercept_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/christian/.cache/pypoetry/virtualenvs/language-classification-training-bA8kuz6F-py3.9/lib/python3.9/site-packages/skl2onnx/common/_container.py:607: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing file sizes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!du -ha ./output/*"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.2M\t./output/classifier.onnx\n",
      "11M\t./output/classifier.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Differences in tokenization and normalization:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vectorizer = TfidfVectorizer(token_pattern=r\"[İ\\w_]{2,}\", lowercase=False)\n",
    "vectorizer.fit(x_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, token_pattern='[İ\\\\w_]{2,}')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tfidf_settings = {\n",
    "    TfidfVectorizer: {\n",
    "        \"tokenexp\": r\"[\\pL\\pN_]{2,}\"\n",
    "    }\n",
    "}\n",
    "initial_type = [('string_input', StringTensorType([None, 1]))]\n",
    "onx = convert_sklearn(vectorizer, initial_types=initial_type, options=tfidf_settings)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import onnxruntime\n",
    "\n",
    "with open(\"data/vectorizer.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n",
    "session = onnxruntime.InferenceSession(\"data/vectorizer.onnx\")\n",
    "inputs = {'string_input': x_test[:1]}\n",
    "pred_onx = session.run(None, {\"string_input\": np.array([\"И с этими словами она села в его карету, и, даже не\"]).reshape(1, 1)})"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vectorizer.onnx'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40250/2732278623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/vectorizer.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/vectorizer.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/vectorizer.onnx'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for t in x_train:\n",
    "    pred_sklearn = np.sum(vectorizer.transform(np.array([t])))\n",
    "    pred_onx = np.sum(session.run(None, {\"string_input\": np.array([t]).reshape(1, 1)}))\n",
    "    if abs(pred_onx - pred_sklearn) > 0.01:\n",
    "        print(t, pred_sklearn, pred_onx)\n",
    "        print(sorted(list(vectorizer.inverse_transform(vectorizer.transform(np.array([t])))[0])))\n",
    "        print(sorted(list(vectorizer.inverse_transform(session.run(None, {\"string_input\": np.array([t]).reshape(1, 1)})[0])[0])))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test prediction with ONNX model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import onnxruntime\n",
    "\n",
    "session = onnxruntime.InferenceSession(str(output_folder / \"classifier.onnx\"))\n",
    "pred_onx = session.run(None, {\"string_input\": np.array([\"И с этими словами она села в его карету, и, даже не\"]).reshape(1, 1)})\n",
    "print(\"predict\", pred_onx[0])\n",
    "print(\"predict_proba\", pred_onx[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predict ['Russian']\n",
      "predict_proba [{'Arabic': 7.943455784698017e-06, 'Danish': 1.0405490684206598e-05, 'Dutch': 8.303594768221956e-06, 'English': 2.679041699593654e-06, 'French': 3.670612613859703e-06, 'German': 1.1265211469435599e-05, 'Greek': 1.0832965926965699e-05, 'Hindi': 1.1893760529346764e-05, 'Italian': 5.484678695211187e-06, 'Kannada': 1.8420292690279894e-05, 'Malayalam': 1.082171183952596e-05, 'Portugeese': 5.10070185555378e-06, 'Russian': 0.9998569488525391, 'Spanish': 5.064754532213556e-06, 'Sweedish': 6.081358151277527e-06, 'Tamil': 1.3298573321662843e-05, 'Turkish': 1.1620059012784623e-05}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for sample in [\"Ein kleiner deutscher Text\", \"A small text without meaning\", \"C'è un pò d'italiano\", \"Une petite histoire de Paris\", \"Генсек ООН призвал к соблюдению перемирия во время Олимпиады\"]:\n",
    "    pred_onx = session.run(None, {\"string_input\": np.array([sample]).reshape(1, 1)})\n",
    "    print(sample, \"=>\", pred_onx[0], pred_onx[1][0][pred_onx[0][0]])\n",
    "    #print(\"predict_proba\", pred_onx[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ein kleiner deutscher Text => ['German'] 0.7684409618377686\n",
      "A small text without meaning => ['English'] 0.9986410140991211\n",
      "C'è un pò d'italiano => ['Italian'] 0.7141642570495605\n",
      "Une petite histoire de Paris => ['French'] 0.9968007802963257\n",
      "Генсек ООН призвал к соблюдению перемирия во время Олимпиады => ['Russian'] 0.991202175617218\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_onnx = session.run(None, {\"string_input\": np.array([s.encode(\"utf-8\") for s in x_test]).reshape(len(x_test), 1)})\n",
    "print(metrics.classification_report(y_test, pred_onnx[0]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      0.98      0.99       106\n",
      "      Danish       0.99      0.96      0.97        73\n",
      "       Dutch       0.99      0.98      0.99       111\n",
      "     English       0.92      1.00      0.96       291\n",
      "      French       1.00      0.99      0.99       219\n",
      "      German       1.00      0.98      0.99        93\n",
      "       Greek       1.00      1.00      1.00        68\n",
      "       Hindi       1.00      1.00      1.00        10\n",
      "     Italian       1.00      0.99      0.99       145\n",
      "     Kannada       1.00      1.00      1.00        66\n",
      "   Malayalam       1.00      0.98      0.99       121\n",
      "  Portugeese       0.99      0.97      0.98       144\n",
      "     Russian       1.00      0.99      0.99       136\n",
      "     Spanish       0.99      0.97      0.98       160\n",
      "    Sweedish       0.99      0.98      0.99       133\n",
      "       Tamil       1.00      0.99      0.99        87\n",
      "     Turkish       1.00      0.98      0.99       105\n",
      "\n",
      "    accuracy                           0.98      2068\n",
      "   macro avg       0.99      0.98      0.99      2068\n",
      "weighted avg       0.99      0.98      0.98      2068\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sample():\n",
    "    sample_size = 1\n",
    "    for i in range(0, len(x_test)-sample_size, sample_size):\n",
    "        yield x_test[i:i+sample_size]\n",
    "\n",
    "def benchmark_onnx():\n",
    "    for t in sample():\n",
    "        pred_onx = session.run(None, {\"string_input\": np.array([t]).reshape(len(t), 1)})\n",
    "\n",
    "def benchmark_sklearn():\n",
    "    for t in sample():\n",
    "        pred = clf_pipeline.predict(np.array(t))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%timeit benchmark_sklearn()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.04 s ± 920 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%timeit benchmark_onnx()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.84 s ± 99.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1369df39e3417fc6e0cf6e36dee8309bdee89bc4efd538bf35008d7b2e1e9fc"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('language-classification-training-bA8kuz6F-py3.9': poetry)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}